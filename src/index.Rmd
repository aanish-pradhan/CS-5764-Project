---
title: "*What* do Neural Networks Learn?"
subtitle: "Subtitle"
author:
    - "Patrick Dewey"
    - "Aanish Pradhan"
date: "May 1, 2024"
output: 
  html_document: 
    mathjax: "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML.js"
    toc: true
    toc_depth: 5
    fig_caption: true
toc-title: "Outline"
header-includes:
    - \usepackage{bookdown}
---

\newcommand{\dataset}[1]{\textcolor{blue}{\texttt{#1}}}

```{r setup, include = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Packages
library(plotly)
library(ggplot2)
library(gapminder)
library(reticulate)
```

```{python Python Packages, echo = FALSE}
import matplotlib.pyplot as plt # Data visualization
import numpy as np
import pandas as pd # Data frame manipulation
import sklearn.datasets as datasets # Datasets
```

Artificial neural networks (ANNs), or neural networks for short, are some of 
the most powerful methods in machine learning. With the resurgence in deep 
learning research starting in 2010, cutting-edge machine learning research has 
gradually shifted to using ANNs as the go-to method for solving problems.  

Neural networks often suffer from being highly unintepretable sometimes being 
called "black box" models, referring to the seemingly magical nature by which 
data goes into black, opaque box where results magically appear on the other 
end. This issue is of serious concern.

# Objectives

We will focus our efforts on classification networks.


# Decision Boundaries

One of the best ways to understand how an ANN is able to classify datapoints of 
different classes is by understanding how ANNs draw decision boundaries.

## The Universal Approximation Theorem

The Universal Approximation theorem is the primary reason why neural networks 
function in the first place. The essence of the theorem is as follows:

*Given an arbitrary, differentiable function $f(x)$ there exists a neural 
network architecture that can approximate $f(x)$ to any desired degree of 
accuracy*

## Example Datasets {.tabset}

To visualize how a neural network learns to separate two classes, we have 
selected three datasets: `biclusters`, `circles` and `moons`. These synthetic 
datasets were generated using the scikit-learn Python package. The two clusters 
of `biclusters` can be optimally separated with a linear decision boundary 
while the clusters in the other datasets require nonlinear boundaries.

### Biclusters

```{python Biclusters Data Generation, echo = FALSE}
biclustersData = datasets.make_blobs(n_samples = 500, n_features = 2, 
	cluster_std = 2, centers = 2, shuffle = True, random_state = 42)
X = pd.DataFrame(biclustersData[0], columns = ['x', 'y'])
y = pd.DataFrame(biclustersData[1], columns = ["cluster"])
biclustersData = pd.concat([X, y], axis = 1)
biclustersData.to_csv("../assets/Datasets/biclusters.csv", sep = ',')

del(biclustersData, X, y)
```


```{r Biclusters Data Visualization, echo = FALSE, fig.align = "center", message = FALSE}
biclustersData <- readr::read_csv("../assets/Datasets/biclusters.csv")
biclustersData$cluster <- as.factor(biclustersData$cluster)

ggplot(biclustersData) + 
	geom_point(aes(x, y, color = cluster)) + 
	labs(title = "Biclusters Data",
		 subtitle = expression(italic('n')~" = "~500~", σ = 2"),
		 x = expression(italic('x')),
		 y = expression(italic('y')),
		 color = "Cluster",
		 caption = "Source: scikit-learn v1.4.2") +
	theme_bw()

rm(biclustersData)
```

<center>
<video width="910" height="720" controls>
  <source src="../assets/Decision-Boundaries/Biclusters/output.mp4" type="video/mp4">
</video>
</center>

### Circles

```{python Circles Data Generation, echo = FALSE}
circlesData = datasets.make_circles(n_samples = 500, shuffle = True, noise = 0.05, random_state = 42)
X = pd.DataFrame(circlesData[0], columns = ['x', 'y'])
y = pd.DataFrame(circlesData[1], columns = ["cluster"])
circlesData = pd.concat([X, y], axis = 1)
circlesData.to_csv("../assets/Datasets/circles.csv", sep = ',')

del(circlesData, X, y)
```

```{r Circles Data Visualization, echo = FALSE, fig.align = "center", message = FALSE}
circlesData <- readr::read_csv("../assets/Datasets/circles.csv")
circlesData$cluster <- as.factor(circlesData$cluster)

ggplot(circlesData) + 
	geom_point(aes(x, y, color = cluster)) + 
	labs(title = "Circles Data",
		 subtitle = expression(italic('n')~" = "~500~", σ = 0.05"),
		 x = expression(italic('x')),
		 y = expression(italic('y')),
		 color = "Cluster",
		 caption = "Source: scikit-learn v1.4.2") +
	theme_bw()

rm(circlesData)
```

<center>
<video width="910" height="720" controls>
  <source src="../assets/Decision-Boundaries/Circles/output.mp4" type="video/mp4">
</video>
</center>


### Moons

```{python Moons Data Generation, echo = FALSE}
moonsData = datasets.make_moons(n_samples = 500, shuffle = True, noise = 0.1, 
	random_state = 42)
X = pd.DataFrame(moonsData[0], columns = ['x', 'y'])
y = pd.DataFrame(moonsData[1], columns = ["cluster"])
moonsData = pd.concat([X, y], axis = 1)
moonsData.to_csv("../assets/Datasets/moons.csv", sep = ',')

del(moonsData, X, y)
```

```{r Moons Data Visualization, echo = FALSE, fig.align = "center", message = FALSE}
moonsData <- readr::read_csv("../assets/Datasets/moons.csv")
moonsData$cluster <- as.factor(moonsData$cluster)

ggplot(moonsData) + 
	geom_point(aes(x, y, color = cluster)) + 
	labs(title = "Moons Data",
		 subtitle = expression(italic('n')~" = "~500~", σ = 0.1"),
		 x = expression(italic('x')),
		 y = expression(italic('y')),
		 color = "Cluster",
		 caption = "Source: scikit-learn v1.4.2") +
	theme_bw()

rm(moonsData)
```

<center>
<video width="910" height="720" controls>
  <source src="../assets/Decision-Boundaries/Moons/output.mp4" type="video/mp4">
</video>
</center>



# Weights and Biases {.tabset}

## MNIST

## CIFAR-10

